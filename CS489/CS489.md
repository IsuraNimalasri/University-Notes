CS489
=====

Special Topics in Computer Science - Introduction to Machine Learning.

    Yaoliang Yu
    Section 001
    Email: yaoliang.yu@uwaterloo.ca
    Website: http://cs.uwaterloo.ca/~y328yu/mycourses/489
    Office Hours: Tuesdays/Thursdays 2:40pm-3:40pm in DC-3617
    Tuesdays/Thursdays 4:00pm-5:20pm


# 7/9/17

Course is potentially going to be collaboration between professor and Focal Systems - guest lectures by deep learning engineers from Focal and assignments from real-world systems. Contact agastya@focal.systems for questions and comments.

Course questions on Piazza, content on LEARN. Course will use MATLAB, Python, or Julia, and requires CS341 concepts.

No required textbooks. 5 semi-weekly assignments worth 50% total, submittable via LEARN, remaining 50% from an open book final exam. There's also a 5% bonus project, consisting of a 1 page proposal and 8 page report on a machine-learning-related project. ;wip: conssider one for robotics control

Machine learning is about giving computers the ability to learn things that they aren't explicitly programmed to do. More specifically, for a machine to **learn** is to, as experience $E$ increases, improve a performance measure $P$ for some class of tasks $T$. Essentially, a program learns if it **gets better at solving a problem as it gains more experience**.

Machine learning falls into three categories:

* Supervised learning: classification/regression/ranking - there's a source of truth that the machine can use to determine the true answer for at least some of the problem instances.
    * Example: the Not Hotdog app - images labelled hotdogs/not-hotdogs are used to train the model, which is then used to make predictions about new images.
    * Given a training set of pairs $\tup{x, y}$, find a function $f: X \to Y$ such that $f(x)$ has good performance on values of $x$ that haven't been seen in the training set.
    * We don't actually care that much about performance in the training set - too-high performance in the training set is overfitting
* Reinforcement learning: control/pricing/gaming - there's no explicit source of truth, but doing something gives feedback, like how good the previous output was.
    * Example: AlphaGo uses a reinforcement learning model to guide monte-carlo tree search - wins give positive feedback, losses give negative feedback.
* Unsupervised learning: clustering - there's no explicit source of truth.
    * Example: Google Youtube clustering 9-layer network from 2012 was trained to cluster objects, and managed to learn to detect faces by itself.

Modern ML research focuses on representation of data (e.g., feature engineering), interpretation of results, generalizing models to different domains (e.g., applying image classifiers to video), time/space complexity, learning efficiency (how many samples do we need? how big does the training set need to be?), and real-world applications.