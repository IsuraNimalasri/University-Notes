<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS489 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",

        // not yet available in KaTeX
        "\\operatorname": "\\mathop{\\text{#1}}\\nolimits", //wip: spacing is slightly off
        "\\not": "\\rlap{\\kern{7.5mu}/}", //wip: slash angle is slightly off
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
      throwOnError: false,
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      katex.render(texText.data, mathElements[i], mathOptions);
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="cs489">CS489</h1>
<p>Special Topics in Computer Science - Introduction to Machine Learning.</p>
<pre><code>Yaoliang Yu
Section 001
Email: yaoliang.yu@uwaterloo.ca
Website: http://cs.uwaterloo.ca/~y328yu/mycourses/489
Office Hours: Tuesdays/Thursdays 2:40pm-3:40pm in DC-3617
Tuesdays/Thursdays 4:00pm-5:20pm</code></pre>
<h1 id="section">7/9/17</h1>
<p>Course is potentially going to be collaboration between professor and Focal Systems - guest lectures by deep learning engineers from Focal and assignments from real-world systems. Contact agastya@focal.systems for questions and comments.</p>
<p>Course questions on Piazza, content on LEARN. Course will use MATLAB, Python, or Julia, and requires CS341 concepts.</p>
<p>No required textbooks. 5 semi-weekly assignments worth 50% total, submittable via LEARN, remaining 50% from an open book final exam. There's also a 5% bonus project, consisting of a 1 page proposal and 8 page report on a machine-learning-related project. ;wip: conssider one for robotics control</p>
<p>Machine learning is about giving computers the ability to learn things that they aren't explicitly programmed to do. More specifically, for a machine to <strong>learn</strong> is to, as experience <span class="math inline">E</span> increases, improve a performance measure <span class="math inline">P</span> for some class of tasks <span class="math inline">T</span>. Essentially, a program learns if it <strong>gets better at solving a problem as it gains more experience</strong>.</p>
<p>Machine learning falls into three categories:</p>
<ul>
<li>Supervised learning: classification/regression/ranking - there's a source of truth that the machine can use to determine the true answer for at least some of the problem instances.
<ul>
<li>Example: the Not Hotdog app - images labelled hotdogs/not-hotdogs are used to train the model, which is then used to make predictions about new images.</li>
<li>Given a training set of pairs <span class="math inline">\tup{x, y}</span>, find a function <span class="math inline">f: X \to Y</span> such that <span class="math inline">f(x)</span> has good performance on values of <span class="math inline">x</span> that haven't been seen in the training set.</li>
<li>We don't actually care that much about performance in the training set - too-high performance in the training set is overfitting</li>
</ul></li>
<li>Reinforcement learning: control/pricing/gaming - there's no explicit source of truth, but doing something gives feedback, like how good the previous output was.
<ul>
<li>Example: AlphaGo uses a reinforcement learning model to guide monte-carlo tree search - wins give positive feedback, losses give negative feedback.</li>
</ul></li>
<li>Unsupervised learning: clustering - there's no explicit source of truth.
<ul>
<li>Example: Google Youtube clustering 9-layer network from 2012 was trained to cluster objects, and managed to learn to detect faces by itself.</li>
</ul></li>
</ul>
<p>Modern ML research focuses on representation of data (e.g., feature engineering), interpretation of results, generalizing models to different domains (e.g., applying image classifiers to video), time/space complexity, learning efficiency (how many samples do we need? how big does the training set need to be?), and real-world applications.</p>
<p>New notation: <span class="math inline">A_i</span> is the <span class="math inline">i</span>-th 1-indexed row of the matrix <span class="math inline">A</span>, and <span class="math inline">A_{:j}</span> is the <span class="math inline">j</span>-th 1-indexed column of the matrix <span class="math inline">A</span>.</p>
<p>New notation: <span class="math inline">\sign x = \begin{cases} 1 &amp;\text{if } x &gt; 0 \\ -1 &amp;\text{if } x &lt; 0 \\ \text{undefined} &amp;\text{if } x = 0 \end{cases}</span>.</p>
<p>New notation: derivative of a function is <span class="math inline">Df(x) = \lim_{\delta \to 0} \frac{f(x + \delta) - f(x)}{\delta}</span>.</p>
<p>New notation: <span class="math inline">\min_{a: f(a), b: g(b), \ldots} f(a, b, c, \ldots)</span> is the minimum value of <span class="math inline">f(a, b, c, \ldots)</span> such that <span class="math inline">f(a), g(b), \ldots</span> are all true. The <span class="math inline">a: f(a)</span> part might also be written as just <span class="math inline">a</span> if there's no constraints.</p>
<p>New notation: <span class="math inline">\argmin_{a: f(a), b: g(b), \ldots} f(a, b, c, \ldots)</span> is the values of <span class="math inline">a, b, c, \ldots</span> such that <span class="math inline">f(a, b, c, \ldots)</span> is minimised and <span class="math inline">f(a), g(b), \ldots</span> are all true. The <span class="math inline">a: f(a)</span> part might also be written as just <span class="math inline">a</span> if there's no constraints.</p>
<h1 id="section-1">12/9/17</h1>
<p>Consider the problem of filtering out spam emails. The training set would be a set <span class="math inline">X</span> of emails (e.g., a vector where each dimension represents a feature, like whether word <span class="math inline">i</span> appears in the email) and a set <span class="math inline">Y</span> representing the spamminess of those emails (e.g., real number between -1 and 1). One of the most important parts of this task is making sure we have a good representation for features in our emails. In a bag of words model, for example, we might make <span class="math inline">X</span> a 10000-dimensional vector where each element represents whether one of 10000 words appears in the email's subject.</p>
<p>In <strong>batch learning</strong>, we care about performance on the testing set <span class="math inline">X&#39;</span>, and the training set is just the means by which we get there, by performing statistics on <span class="math inline">X</span> and assuming things about <span class="math inline">X&#39;</span>. In <strong>online learning</strong>, data is received in a streaming fashion - we need to product the value of <span class="math inline">y</span> without knowing its true value.</p>
<p>In this course, we'll use <span class="math inline">&lt;a, b&gt;</span> to represent the inner product <span class="math inline">a \cdot b = a^T b</span>. Also, <span class="math inline">\sign(x)</span> is 1 when <span class="math inline">x &gt; 0</span>, -1 when <span class="math inline">x &lt; 0</span>, and undefined when <span class="math inline">x = 0</span> (some other courses will instead define it to be 0).</p>
<h2 id="perceptrons">Perceptrons</h2>
<p>The perceptron is a machine learning model based on a highly simplified model of a neuron. It takes in activation from neighboring neurons, takes their weighted sum, and then applies the activation function to them, the <span class="math inline">\sign</span> function, which is the neuron's output. We'll study Rosenblatt's original design from 1958, along with several additions and improvements made since then.</p>
<p>Perceptrons are used for <strong>binary classification problems</strong>. We are given a training set <span class="math inline">\set{\tup{\vec x_1, y_1}, \tup{\vec x_2, y_2}, \ldots}</span> and a testing set <span class="math inline">\set{\vec t_1, \vec t_2, \ldots}</span> where <span class="math inline">\vec x_i, \vec t_i</span> are feature vectors, and <span class="math inline">y_i</span> is the binary <strong>category</strong>, either -1 or 1. Using the training set, we want to train the perceptron to determine the category <span class="math inline">y_i</span> for each <span class="math inline">\vec t_i</span>.</p>
<p>A perceptron is simply <span class="math inline">y = \sign(\vec w \cdot \vec x + b)</span>, where <span class="math inline">\vec w</span> is the perceptron's <strong>weights vector</strong>, <span class="math inline">b</span> is the perceptron's <strong>bias</strong>, <span class="math inline">\vec x</span> is the <strong>input</strong>, and <span class="math inline">y \in \set{-1, 1}</span> is the <strong>prediction</strong>. Note that <span class="math inline">\vec w + b</span> should be a <strong>hyperplane</strong> separating the positive values of <span class="math inline">y_i</span> from the negative values of <span class="math inline">y_i</span>, and the sign of <span class="math inline">\vec w \cdot \vec x + b</span> determines which side of the hyperplace the point <span class="math inline">\vec x</span> is on (the positive predictions side, or the negative predictions side). For now, let's assume that for the training set, there exists a hyperplane that separates all of the positives from all of the negatives - that the data is <strong>separable</strong>.</p>
<p>Now we'll try to simplify the perceptron formula to make it easier to work with. First, let's get rid of the <span class="math inline">\sign</span> by multiplying both sides of the perceptron formula by <span class="math inline">y</span>: <span class="math inline">y^2 = y \sign(\vec w \cdot \vec x + b)</span>, and since <span class="math inline">y</span> is either -1 or 1, <span class="math inline">y^2 = 1</span>, so <span class="math inline">y \sign(\vec w \cdot \vec x + b) = 1</span>, or in other words, <span class="math inline">y (\vec w \cdot \vec x + b) &gt; 0</span>. Expand to get <span class="math inline">\vec w \cdot (y\vec x) + by &gt; 0</span></p>
<p>Let <span class="math inline">\vec w&#39; = \begin{bmatrix} \vec w \\ b \end{bmatrix}</span> and <span class="math inline">a = \begin{bmatrix} y \vec x \\ y \end{bmatrix}</span> - we've chosen these definitions specifically so that <span class="math inline">\vec w \cdot (y\vec x) + by &gt; 0</span> is equivalent to <span class="math inline">a \cdot w&#39; &gt; 0</span>, and so that the value of <span class="math inline">\vec w&#39;</span> represents the perceptron parameters exactly.</p>
<p>When training the perceptron, our goal is to fit the hyperplane to our training set. That means we'll want to make perceptron predictions in bulk, so it would be nice to be able to represent that in a compact way. To do this, we'll let <span class="math inline">A = \begin{bmatrix} \vec a_1 &amp; \vec a_2 &amp; \ldots \end{bmatrix}</span>, where <span class="math inline">\vec a_i = \begin{bmatrix} y_i \vec x_i \\ y_i \end{bmatrix}</span> - columns of <span class="math inline">A</span> are values of <span class="math inline">\vec a</span> corresponding to each value of <span class="math inline">\vec x_i</span>. Written out fully, that's <span class="math inline">A = \begin{bmatrix} y_1 \vec x_1 &amp; y_2 \vec x_2 &amp; \ldots \\ y_1 &amp; y_2 &amp; \ldots \end{bmatrix}</span>.</p>
<p>Clearly, <span class="math inline">A^T \vec w&#39; &gt; 0</span> is equivalent to <span class="math inline">\forall \vec x_i, \sign(\vec w&#39; \cdot \vec x_i + b) = y</span>. We've now simplified the perceptron problem down to a single matrix multiplication and a comparison! Now, <span class="math inline">\vec w&#39;</span> contains all the perceptron parameters, and the columns of <span class="math inline">A</span> are the data points (each with a trailing 1 element), premultiplied by the label.</p>
<p>Now the problem becomes: given premultiplied data in a matrix <span class="math inline">A</span>, find <span class="math inline">\vec w&#39;</span> such that <span class="math inline">A^T \vec w&#39; &gt; 0</span>. The <strong>perceptron training algorithm</strong> does this, and works as follows: repeatedly choose a column <span class="math inline">\vec a_i</span> of <span class="math inline">A</span>, and if <span class="math inline">\vec a_i \cdot \vec w&#39; \le 0</span>, change <span class="math inline">\vec w&#39;</span> by adding <span class="math inline">\vec a_i</span> to it. Stop when <span class="math inline">\vec a_i \cdot \vec w&#39; &gt; 0</span> for all <span class="math inline">\vec a_i</span> in <span class="math inline">A</span>, or when we reach an iteration/passes limit.</p>
<p>Why do we correct the weights when <span class="math inline">\vec a_i \cdot \vec w&#39; \le 0</span> by adding <span class="math inline">\vec a_i</span> to <span class="math inline">\vec w&#39;</span>? Well, the next time we choose the <span class="math inline">\vec a_i</span> column, we'll get <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i) = \vec a_i \cdot \vec w&#39; + \magn{\vec a_i}^2</span>. Since <span class="math inline">\magn{\vec a_i}^2 &gt; 0</span>, <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i) &gt; \vec a_i \cdot \vec w&#39;</span>, so <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i)</span> is closer to being positive.</p>
<p>(Python implementation not included, since this is a question in assignment 1)</p>
<p>After training, we can make predictions for any given input <span class="math inline">\vec x</span> with the usual formula, <span class="math inline">y = \sign\left(\vec w&#39; \cdot \begin{bmatrix} \vec x \\ 1 \end{bmatrix}\right)</span>.</p>
<p>This algorithm is very simple to implement, yet works quite well in practice. Also, the fact that its formula is a linear combination is interesting. If we look at the weights, we notice that large positive weights mean that the corresponding feature strongly suggests that the prediction should be positive, whereas large negative weights strongly suggest that the prediction should be negative.</p>
<p>How well does a perceptron converge when running the training algorithm described above? <strong>Block's perceptron convergence theorem</strong> gives us an idea. If <span class="math inline">A</span> is separable (i.e., a hyperplane exists that separates positive cateogry points from negative category points), then <span class="math inline">\vec w&#39;</span> will converge to some <span class="math inline">\vec w^*</span>. If every column of <span class="math inline">A</span> is selected indefinitely often, then <span class="math inline">A^T \vec w^* &gt; 0</span>. Furthermore, if <span class="math inline">\vec w&#39; = \vec 0</span> initially, then the perceptron converges after at most <span class="math inline">(R / \gamma)^2</span> iterations, where <span class="math inline">R = \max\set{\magn{a_1}, \magn{a_2}, \ldots}</span> and <span class="math inline">\gamma = \max\set{\min\set{\vec w \cdot \vec a_1, \vec w \cdot \vec a_2, \ldots} : \magn{\vec w} \le 1}</span> (the margin - the minimum distance between the convex hull of the positive points and the negative points). Essentially, the margin represents the distance between the &quot;hardest&quot; two datapoints to classify.</p>
<p>Note that these values of <span class="math inline">R</span> and <span class="math inline">\gamma</span> are purely functions of the dataset, and that they don't directly depend on the size of <span class="math inline">A</span> and the number of dimensions <span class="math inline">d</span>. In other words, the number of mistakes the perceptron makes would be independent of the dataset size and number of dimensions! The larger the margin is, the faster the perceptron converges. Block's perceptron convergence theorem gives us a worst case bound, but in many practical situations the perceptron will perform a lot better.</p>
<p>Also, the perceptron stops at an arbitrary linear separator that correctly separates the points, not necessarily the one that most cleanly separates the positive and negative points (with the largest possible minimum distance from the hyperplane to positive/negative predictions). In fact, the resulting hyperplane will even depend on the order we feed in the data. We can use support vector machines instead to find that hyperplane (which also happens to be unique for each dataset!). This is the main disadvantage of perceptrons - they might only barely separate the training data, so they're less robust to unseen data than those that find a linear separator with a larger margin.</p>
<p>If the data is <strong>not separable</strong>, Block's perceptron convergence theorem doesn't apply anymore. The <strong>perceptron boundedness theorem</strong> says that convergence is only guaranteed if such a hyperplane exists, but if it doesn't, then the iterations are still bounded, because the perceptron's state will start cycling after a certain number of iterations. In practice, this means we would specify a time or iteration limit when doing training, or when the training/validation error stops changing, or even if weights stop changing much when using diminishing step sizes.</p>
<p>If we end up with non-separable data, we might want to find a better feature representation, use a deeper model, or use a <strong>soft margin</strong> - instead of a hyperplane that perfectly separates positive/negative values of <span class="math inline">y</span>, we can allow a few mistakes.</p>
<p>There are many ways to extend perceptrons to classify things into more than two categories (positive/negative). One way is <strong>one vs. all</strong>, where we have one perceptron per category, perceptron with highest activation level wins - <span class="math inline">\max_c(w_c \cdot x)</span>. The issue with this is that it's imbalanced - each perceptron has to give negative predictions far more often than positive ones, since it only gives positive prediction for its own category and otherwise must give a negative prediction. Another is <strong>one vs. one</strong>, where we have one perceptron for every pair of categories, where a positive prediction means the datapoint is in the first category and negative means the other category, and then take a vote to find the most commonly predicted category as the final answer.</p>
<p>An example of applying perceptrons online is pricing - selling a product to the user at a price <span class="math inline">y</span>, and updating weights if the price is too high and the user doesn't buy the product.</p>
<h1 id="section-2">14/9/17</h1>
<p>Assignment 1 now available, due in two weeks.</p>
<p>A <strong>pass</strong> is a run through all of the training data - 100 passes means we go through the training data 100 times. An <strong>iteration</strong> is a run through a single data point in our training data.</p>
<h2 id="linear-regression">Linear Regression</h2>
<p>Consider a scatter plot of house market value vs. square footage. We'd expect that these two are pretty well correlated. A linear regression over these two variables can be used to give us a line of best fit.</p>
<p>Regression problems are about fitting models to match datasets as closely as possible. Linear regression problems try to fit linear models to datasets. When we're doing regression problems, we have to consider whether to use linear/nonlinear models, and whether we'll be using it to interpolate or extrapolate (choosing the perfect model is much more important for extrapolation)</p>
<p>Formally, a regression problem is: find <span class="math inline">f(\vec x) \approxeq \vec y</span> given <span class="math inline">\vec x</span> (the <strong>feature vector</strong> a real vector) and <span class="math inline">y</span> (the <strong>response value</strong>, a real number). The hard part of this is that <span class="math inline">\vec x</span> and <span class="math inline">y</span> are drawn from unknown distributions, which makes it hard to interpolate/extrapolate. Additionally, we need a way to express how much error there is in our model predictions - a <strong>loss function</strong>.</p>
<p>One family of regression algorithms is <strong>risk minimizers</strong> (expected loss minimizers): algorithms that try to find <span class="math inline">f</span> such that <span class="math inline">\min_{f: \vec x \to y} E[L(f(\vec x), y)]</span>, where <span class="math inline">L</span> is the loss function.</p>
<p>A common loss function is <strong>least squares</strong>: <span class="math inline">\min_{f: \vec x \to y} E[\magn{f(\vec x) - y}^2]</span>. The correct loss function for a given situation is often hard to determine, so we use one that's simple and efficient to compute - least squares works well enough for most situations. Additionally, of all the minimizers of $_W _F, <span class="math inline">W = A^+ CB^+</span>$ is the one with the smallest F-norm, where <span class="math inline">A^+</span> is the pseudo-inverse of <span class="math inline">A</span> (Sondermann '86, Yu &amp; Shuurmans '11) - this is mostly a theoretical result, but gives us another good reason to use least squares loss.</p>
<p>Clearly, <span class="math inline">E[\magn{f(\vec x) - y}^2] = E[\magn{f(\vec x) - E(y \mid \vec x)}^2] + E[\magn{E(y \mid \vec x) - y}^2]</span>. Note that the second term doesn't really depend on <span class="math inline">f</span> - it's the <strong>inherent noise variance</strong>, the noise that we can't get rid of no matter how good our regression function is. Also, the first term gives us the problem in a much simpler form: we want to find an <span class="math inline">f(\vec x)</span> that approximates <span class="math inline">E(y \mid \vec x)</span> well, to make this term smaller.</p>
<p>One way to make this optimization process easier is to assume that <span class="math inline">f(\vec x)</span> is linear, so <span class="math inline">f(\vec x) = E(\vec y \mid \vec x) = A \vec x + \vec b</span> for some matrix <span class="math inline">A</span>. If we make this assumption, then with risk minimization we're trying to find <span class="math inline">\min_{f: \vec x \to \vec y} E[A \vec x + \vec b - \vec y]</span>. We can't minimize this directly because we don't know the true distribution of the variables, but using the law of large numbers, <span class="math inline">\frac 1 n \sum Z_i = E(Z)</span> for any <span class="math inline">Z = \set{Z_1, Z_2, \ldots}</span>. So if we assume the model is linear, and the sample is large, then the risk minimization can be approximated by <span class="math inline">\min_{\vec a, \vec b} \frac 1 n \sum \magn{A \vec x + \vec b - \vec y}^2</span> (this approximation is called the <strong>empirical risk</strong>).</p>
<p>Let's simplify the <span class="math inline">\min_{\vec a, \vec b} \frac 1 n \sum \magn{A \vec x + \vec b - \vec y}^2</span> approximation, using something very similar to what we did for perceptrons. First, let's define <span class="math inline">W = \begin{bmatrix} A^T \\ {\vec b}^T \end{bmatrix}</span> and <span class="math inline">\vec x&#39; = \begin{bmatrix} \vec x \\ 1 \end{bmatrix}</span>. Now we have <span class="math inline">\min_W \frac 1 n \sum \magn{W^T \vec x&#39; - \vec y}^2</span>, which is slightly shorter/cleaner.</p>
<p>Let <span class="math inline">\vec x_i</span> be the <span class="math inline">i</span>th value of <span class="math inline">\vec x</span> in our training set. Just like for the perceptrons simplifications above, we also want to include all of the training set data points in a single expression, to make our minimization problem simpler. To do this, let <span class="math inline">X = \begin{bmatrix} {\vec x_1&#39;}^T \\ {\vec x_2&#39;}^T \\ \vdots \end{bmatrix}, Y = \begin{bmatrix} {\vec y_1}^T \\ {\vec y_2}^T \\ \vdots \end{bmatrix}</span>. Now, we can write this as <span class="math inline">\min_W \magn{XW - Y}_F^2</span> where <span class="math inline">\magn{A}_F = \sum_{i, j} A_ij</span> is the <strong>Frobenius norm</strong> - each element simply gets squared and the squares are all summed together to get the result, like the Euclidean norm, but extended for any matrix.</p>
<p>The <strong>least squares problem</strong> is now writeable as <span class="math inline">\min_W \magn{XW - Y}_F^2</span>, and we're minimizing the <strong>sum of square residuals</strong> <span class="math inline">XW - Y</span> (sum of square distances between the predicted values and true values). Here, <span class="math inline">Y</span> is a matrix with columns as the true responses, and the residuals are the distances between each true response in <span class="math inline">Y</span> and the point that the hyperplane would predict given <span class="math inline">X</span>.</p>
<p>Note that the Frobenius norm can be defined as: <span class="math inline">\magn{A}_F^2 = \trace{A^T A}</span>. Additionally, the following are identities: <span class="math inline">\trace(A + B) = \trace(A) + \trace(B)</span>, <span class="math inline">\trace(AB) = \trace(BA)</span>, <span class="math inline">\trace(A) = \trace(A^T)</span>, and <span class="math inline">\trace(cA) = c \trace(A)</span>.</p>
<p>Therefore, <span class="math inline">\magn{XW - Y}_F^2 = \trace((XW - Y)^T (XW - Y)) = \trace((W^T X^T - Y^T) (XW - Y)) = \trace(W^T X^T X W - Y^T X W - W^T X^T Y + Y^T Y) = \trace(W^T X^T X W) - \trace((Y^T X W)^T) - \trace(W^T X^T Y) + \trace(Y^T Y) = \trace(W^T X^T X W) - \trace(W^T X^T Y) - \trace(W^T X^T Y) + \trace(Y^T Y) = \trace(W^T X^T X W - 2 W^T X^T Y + Y^T Y)</span>. Clearly, this is a quadratic equation with respect to <span class="math inline">W</span>, and we want to find its minimum.</p>
<p>Consider <span class="math inline">\min_x f(x)</span>. Fermat's theorem says that at the minimum <span class="math inline">x</span>, the derivative of <span class="math inline">f(x)</span> must be 0. Consider a general quadratic function <span class="math inline">f(x) = \vec x^T A \vec x + \vec x^T \vec b + c</span>. The derivative is then <span class="math inline">\frac{\dee f(x)}{\dee x} = (A + A^T)\vec x + \vec b</span>.</p>
<p>Note that <span class="math inline">\magn{XW - Y}_F^2 = W^T(X^T X) W - 2W^T X^T Y + Y^T Y</span> (a quadratic equation), and if set the derivative of this to 0 and solve we get <span class="math inline">X^T X W = X^T Y</span> as a solution, which is just a linear system - we have <span class="math inline">X</span> and <span class="math inline">Y</span>, so we can solve for <span class="math inline">W</span>. Note that <span class="math inline">X^T X</span> might be invertible, but we should still never solve for <span class="math inline">W</span> by using <span class="math inline">W = (X^T X)^{-1} X^T Y</span>, since this involves solving <span class="math inline">n</span> linear systems, whereas we can solve it by solving only 1 linear system (in practice, we should almost never actually compute matrix inverses).</p>
<p>Once we have <span class="math inline">W</span>, we can make predictions for any given <span class="math inline">X</span> using <span class="math inline">\hat Y = XW</span>, or evaluate those predictions with <span class="math inline">(Y - \hat Y)^2</span>. We can also evaluate using a different loss function, a technique often used in calibration theory.</p>
<p>Linear regression is disproportionally affected by large outliers. To mitigate this, we sometimes use Huber loss, which is linear for large differences and quadratic for smaller ones, where &quot;larger&quot; and &quot;smaller&quot; are defined by a threshold <span class="math inline">\delta</span>. This ensures overly large outliers don't impact the result too much. Huber's loss function is defined as <span class="math inline">H(\hat y, y) = \begin{cases} \frac 1 2 (\hat y - y)^2 &amp;\text{if } \abs{\hat y - y} \le \delta \\ \delta(\abs{\hat y - y} - \frac{\delta}{2}) &amp;\text{otherwise} \end{cases}</span>.</p>
<p>;wip: talk about regularization</p>
<p><strong>Ill-posed problems</strong> are those that don't have exactly one solution (zero or more than one solution) or don't have their solutions change continuously with respect to the problem initial conditions (i.e., derivative of solution with respect to initial condition doesn't always exist). To handle this sort of regression task, we can use <strong>Tiknohov regularization</strong>. To do this, we just add a term to the formula: <span class="math inline">\min_W \magn{XW - Y}_F^2 + \lambda \magn{W}_F^2</span>, or equivlaently, <span class="math inline">(X^T X + \lambda I)W = X^T Y</span>. A small positive lambda ensures that instead of a small change in the input resulting in a huge difference in the output, it would result in a difference proportional to <span class="math inline">\frac{1}{\lambda}</span> instead. Another way to handle ill-posed problems is to use data augmentation - essentially, adding more data points to make the data appear more regular.</p>
<p>How do we choose hyperparameters like <span class="math inline">\lambda</span> for Tiknohov regularization? We have a training set (for model training), testing set (which we don't see until the end), and sometimes a small validation set (for tuning parameters), and on the training set, we can apply <span class="math inline">n</span>-fold <strong>cross-validation</strong>. Suppose we have <span class="math inline">k</span> different values of <span class="math inline">\lambda</span> we want to consider:</p>
<ol type="1">
<li>Split the training set into <span class="math inline">n</span> roughly-equal sized chunks.</li>
<li>For each value of <span class="math inline">\lambda</span> we want to cross-validate:
<ol type="1">
<li>For each chunk <span class="math inline">i</span>:
<ol type="1">
<li>Train the model on the dataset formed by combining the <span class="math inline">n - 1</span> chunks that are not chunk <span class="math inline">i</span>.</li>
<li>Evaluate the model against the training data in chunk <span class="math inline">i</span>.</li>
</ol></li>
<li>Average the evaluation scores from each chunk to get the average cross-validation score for the value of <span class="math inline">\lambda</span>.</li>
</ol></li>
<li>Pick the value of <span class="math inline">\lambda</span> that has the best average cross-validation score.</li>
</ol>
<h1 id="section-3">19/9/17</h1>
<p>Guest lecture by Francois from Focal Systems (francois@focal.systems).</p>
<p>Almost any ML problem falls into regression or classification.</p>
<p>For linear regression, we're assuming that the response variable <span class="math inline">y</span> is approximated by <span class="math inline">\vec \theta \cdot \vec x + N(0, \sigma)</span>, where <span class="math inline">N(0, \sigma)</span> is a normal distribution centered around 0 with standard deviation <span class="math inline">\sigma</span>. Further overview of some real-world details in implementing linear regression.</p>
<p>Though linear regression is simplistic, it turns out that it works very well in practice, since more complex models require more advanced ways to do regularization and get decent weight vectors. Tools like SVM are used a lot in the real world to model real phenomena, even when they aren't necessarily linear, because it works well enough for most purposes.</p>
<p>Most modern ML problems use SGD - stochastic gradient descent.</p>
<p>A Bernoulli model predicts <span class="math inline">y = P(Y_1 = y_1, \ldots, Y_n = y_n \mid X_1 = x_1, \ldots, X_n = x_n)</span>. If we assume that the distribution of the variables are a Bernoulli distribution, so they're independent, we can then write this as <span class="math inline">y = \prod P(Y_i = y_i \mid X_i = x_i) = \prod p(x_i; w)^{y_i} (1 - p(x_i; w))</span> ;wip: get the formula for this</p>
<p>Logistic regression tries to predict the value of <span class="math inline">0 \le y \le 1</span> given <span class="math inline">\vec x</span> by fitting the formula <span class="math inline">\frac 1 {1 + \exp(\vec w \cdot x)}</span>, where <span class="math inline">\vec w</span> is the thing that we're trying to fit.</p>
<p>We use a sigmoid rather than, say, a step function, because the gradient doesn't have any signal - if we differentiate it, the derivative is just 0 everywhere, so gradient descent wouldn't be able to get closer to the solution at every step. Instead, the sigmoid formula has a gentle curve, so its derivative is more suitable for performing gradient descent on.</p>
<p>Our loss function is then <span class="math inline">f(y&#39;, y) = \ln(y&#39;) * y + \ln(1 - y&#39;) (1 - y)</span>, where <span class="math inline">y&#39;</span> is the model's prediction and <span class="math inline">y</span> is the true value. ;wip: why??? look at slides</p>
<p>Tensorflow example, implementing logistic regression using the built in gradient descent optimizer to minimize the loss function. When doing gradient descent, we want the largest learning rate that still converges.</p>
<p>;wip: logistic regression</p>
<h1 id="section-4">21/9/17</h1>
<p>Guest lecture by Aghastya from Focal Systems (aghastya@focal.systems).</p>
<p>A perceptron tells you which side of a hyperplane a point is, and also includes an algorithm to separate two classes with a hyperplane - a binary classifier. Logistic regression finds a line of best fit, as well as a measure of confidence that our prediction is correct, because the prediction's value is between -1 and 1 rather than exactly -1 or 1 - a binary classifier as well. The logistic regression gives a higher-magnitude prediction the farther it is from the logistic regression's hyperplane.</p>
<p>Perceptrons aren't very good binary classifiers overall for reasons weve previously discussed, but they're computationally cheap to run and easy to reason about. One example of the this is the XOR problem - a hyperplane cannot separate an XOR function: two classes <span class="math inline">\set{\tup{0, 1}, \tup{1, 0}}</span> and <span class="math inline">\set{\tup{0, 0}, \tup{1, 1}}</span>.</p>
<p>The goal of deep learning classification is to learn a representation of the input into something that's linearly separable, so we could then use classifier techniques like logistic regression.</p>
<p>Consider a <strong>two-payer perceptron</strong>. We have parameters <span class="math inline">U, \vec c, \vec w, b</span>. Let <span class="math inline">\vec z = U\vec x + \vec b</span> and <span class="math inline">h = f(\vec z)</span>, where <span class="math inline">f</span> is a nonlinear function like <span class="math inline">x^2</span> or <span class="math inline">\arctan(x)</span>. Then the output of the two-layer perceptron is then <span class="math inline">\hat y = \vec h \cdot \vec w + b</span>. There are three layers here:</p>
<ol type="1">
<li>The <strong>linear layer</strong> takes the model input <span class="math inline">\vec x</span> and transforms it into another linear space via weights <span class="math inline">U</span> and the bias term <span class="math inline">\vec c</span>, to get the hidden layer inputs <span class="math inline">\vec z</span>.
<ul>
<li>This is the part of the perceptron before the <span class="math inline">\sgn</span> function.</li>
</ul></li>
<li>The <strong>hidden layer</strong> takes the hidden input <span class="math inline">\vec z</span> and applies a non-linear function to it, so we can represent non-linearity in the input, to get the linear layer inputs <span class="math inline">\vec h</span>.
<ul>
<li>The function used in these hidden layers is known as an <strong>activation function</strong>.</li>
<li>Common activation functions are sigmoid <span class="math inline">\frac{1}{1 + \exp(x)}</span>, inverse tangent <span class="math inline">\arctan(x)</span>, and rectified linear unit (ReLU) <span class="math inline">\max(0, x)</span>.</li>
</ul></li>
<li>The <strong>linear layer</strong> takes the hidden layer output <span class="math inline">\vec h</span> and transforms it into another linear space via weights <span class="math inline">\vec w</span>.</li>
</ol>
<p>Essentially, we're weighting the input <span class="math inline">\vec x</span> by the weight matrix <span class="math inline">U</span> and the bias term <span class="math inline">\vec c</span> (a value unaffected of input that influences the output in a particular direction) in a <strong>linear layer</strong> to get the <strong>hidden layer inputs</strong> <span class="math inline">\vec z</span>. Then, we apply the non-linearity to those to get the <strong>hidden layer output</strong> <span class="math inline">\vec h</span>, which is also the inputs to the next linear layer, which is a linear classifier. ;wip: explain this better</p>
<p>The idea is to compose a bunch of non-linear functions to learn an arbitrary function, and then use that composition of non-linear functions so that the result is linearly separable. The non-linear functions add complexity to the model, and by adding enough of those non-linear functions, we can represent a lot of complex things.</p>
<p>The <strong>width</strong> of a multi-layer perceptron is the number of dimensions of <span class="math inline">\vec x</span>. Note that this doesn't include the bias term <span class="math inline">\vec c</span>.</p>
<p>We can't just use linear functions because composing linear functions just makes another linear function - if we applied linear functions on a dataset that wasn't linearly separable, it would still not be linearly separable, but if we apply non-linear functions, it may end up linearly separable.</p>
<p>Consider now an example: <span class="math inline">U = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}, \vec c = \begin{bmatrix} 0 \\ -1 \end{bmatrix}, \vec w = \begin{bmatrix} 2 \\ -4 \end{bmatrix}, \vec b = -1, f(x) = \max(t, 0)</span>. The activation function is ReLU (rectified linear unit). As it turns out, this can correctly classify the output of the XOR function.</p>
<p>We can stack these two-layer perceptrons - feeding the output of one into the input of another. It's actually been proven that if the activation function is not a polynomial, and there are enough layers, the resulting model can theoretically learn any function that has values between 0 and 1!</p>
<p>The multi-layer perceptron learns a <strong>hierarchical non-linear feature representation, which we can then apply our existing classification/regression tools to</strong>.</p>
<p>As we go from levels closer to the input to levels farther away, we start being able to classify higher-and-higher-level features. For example, an image classifier network might start with learning lines/circles in the lower levels, and then shapes like trees and cars and faces in the higher levels.</p>
<p>Choosing the right activation function and architecture is currently more of an art than a science - there's a lot of nuances that we'll go over in later lectures. Usually, we'll just add more layers than we need, and regularize afterward.</p>
<h2 id="backpropagation">Backpropagation</h2>
<p>The multilayer perceptron's output is clearly a function of the input, <span class="math inline">\hat y = q(\vec x; \Theta)</span>, where <span class="math inline">\Theta</span> is the parameters of the model. We usually don't know the true function <span class="math inline">q^*</span> that we're trying to approximate, but deep learning samples a lot of values of <span class="math inline">q^*</span> and then tries to approximate that by composing lots of nonlinear functions to get <span class="math inline">q</span>.</p>
<p>To do machine learning, we need to be able to talk about the difference between the prediction <span class="math inline">\hat y</span> and the truth <span class="math inline">y</span> - a loss function <span class="math inline">l(\Theta)</span>. We already looked at cross-entropy, but for now we'll just use the L2 norm <span class="math inline">(\hat y - y)^2</span> ;wip: are those the same things?</p>
<p>Gradient descent is a method of minimizing any function by iteratively taking steps toward the minima.</p>
<p>We want to minimize our loss function using gradient descent - minimizing <span class="math inline">L(\Theta) = \frac 1 n \sum l(q(x_i; \Theta), y_i)</span> by iteratively computing <span class="math inline">\Theta_{t + 1} = \Theta_t - \eta_t \frac{\dee}{\dee \Theta_t} L(\Theta_t)</span>. Here, <span class="math inline">\Delta L(\Theta)</span> is the <strong>gradint</strong>, and <span class="math inline">\eta_t</span> is the <strong>learning rate</strong>.</p>
<p>The loss function should ideally get smaller as we get closer to a good answer, but really we just need a function <span class="math inline">l(x&#39;, x)</span> (<span class="math inline">x&#39;</span> is the prediction, <span class="math inline">x</span> is the true value) that's 0 when we have a correct answer, and positive otherwise.</p>
<p>If the function is continuous, the gradient is the derivative of the function. Note that since we're subtracting the gradient, the gradient actually points away from the direction we're optimizing toward.</p>
<p>The gradient at a particular node in the computation graph is known as a local gradient.</p>
<p>;wip: rewrite this section more clearly, talk about forward and backward pass</p>
<p>Backpropagation is a method of computing the gradient of <span class="math inline">\Theta</span> with respect to <span class="math inline">l(\Theta)</span>. Suppose we have <span class="math inline">r = f(q(x))</span>. By the chain rule, <span class="math inline">\frac{\dee r}{\dee x} = \frac{r}{\dee f} \frac{f}{\dee q} \frac{q}{\dee x}</span>, so we can easily recursively differentiate to get the gradient.</p>
<p>Backpropagation is implemented by building a computation graph, differentiating at the output layer, and working our way back to the input layer. The innovation is that it's much faster than the naive way of computing the gradient, where we hold all but one variable constant and find the partial derivative.</p>
<p>Backpropagation is an expensive algorithm, and was the reason neural networks weren't practical for a long time, until we can cheap GPU-based computation.</p>
<h1 id="section-5">26/9/17</h1>
<p>Assignment 1 deadline extended to Thursday. In-class overview of the assignment.</p>
<p>For the winnow algorithm, normalizing is pretty important, since the step size depends on the maximum absolute value of the data, unlike the perceptron, where those initial parameters don't matter as much.</p>
<p>The winnow algorithm will always converge, regardless of the step size. This is because of the normalization step in the loop, where we divide by the sum of the weights and biases. However, certain step size values will minimize the number of passes before we get convergence.</p>
<p>Closed-form solution for alternating minimization on ridge regression:</p>
<blockquote>
<p>We want to solve <span class="math inline">z^* = \argmin_z \frac 1 2 \magn{\vec X_{:j} z + \sum_{k \ne j} \vec X_{:k} w_k - \vec y}^2 + \lambda z^2</span>.<br />
Let <span class="math inline">\vec u = \vec X_{:j}</span> and <span class="math inline">\vec v = \sum_{k \ne j} \vec X_{:k} w_k - \vec y</span>, so <span class="math inline">z^* = \argmin_z \frac 1 2 \magn{\vec u z + \vec v}^2 + \lambda z^2 = \argmin_z \frac 1 2 \magn{\vec u}^2 z^2 + \vec u \cdot \vec v z + \frac 1 2 \magn{v}^2 + \lambda z^2 = \argmin_z (\frac 1 2 \magn{\vec u}^2 + \lambda) z^2 + \vec u \cdot \vec v z + \frac 1 2 \magn{v}^2</span>.<br />
Since <span class="math inline">\frac 1 2 \magn{v}^2</span> doesn't depend on <span class="math inline">z</span>, it doesn't affect the value of <span class="math inline">\argmin_z</span>. So <span class="math inline">z^* = \argmin_z (\frac 1 2 \magn{\vec u}^2 + \lambda) z^2 + \vec u \cdot \vec v z</span>.<br />
Clearly, <span class="math inline">\frac{\dee}{\dee z} \left(\left(\frac 1 2 \magn{\vec u}^2 + \lambda\right) z^2 + \vec u \cdot \vec v z\right) = \left(\magn{\vec u}^2 + 2\lambda\right)z + \vec u \cdot \vec v</span>.<br />
If we set the derivative to 0, then <span class="math inline">z = -\frac{\vec u \cdot \vec v}{\magn{\vec u}^2 + 2\lambda}</span>. Since the equation was quadratic, this is the only local optima, so it is also the global minimum. Therefore, <span class="math inline">z</span> is minimized at <span class="math inline">z = -\frac{\vec u \cdot \vec v}{\magn{\vec u}^2 + 2\lambda}</span>.</p>
</blockquote>
<h2 id="nearest-neighbor-rule">Nearest neighbor rule</h2>
<p>Supose we have a perceptron classifier <span class="math inline">\hat y = \sgn(\vec x \cdot \vec w + b)</span>. The <strong>decision boundary</strong> <span class="math inline">\vec x \cdot \vec w + b = 0</span> is the decision boundary here, and we can do a lot with it. This is a <strong>parametric</strong> classifier, because it uses a finite-dimensional set of parameters <span class="math inline">\vec w</span>. In contrast, a <strong>non-parametric</strong> classifier has an arbitrarily large number of parameters. The nearest-neighbor rule is non-parametric, because</p>
<p>The <strong>nearest-neighbor rule</strong> is a way to predict <span class="math inline">y</span> given a feature vector <span class="math inline">\vec x</span> and a training set. Given a feature vector <span class="math inline">\vec x</span> (the <strong>query point</strong>), find the training set entry <span class="math inline">\tup{\vec x&#39;, y&#39;}</span> such that <span class="math inline">\vec x&#39;</span> is the nearest to <span class="math inline">\vec x</span> by some distance metric, then predict that <span class="math inline">y = y&#39;</span>. Essentially, we take the nearest point to the given one, and simply take that point's value as the prediction.</p>
<p>The distance metric <span class="math inline">d(x_1, x_2)</span> must be symmetic (<span class="math inline">d(x_1, x_2) = d(x_2, x_1)</span>), definite (<span class="math inline">d(x, x) = 0</span>), and satisfy the triangle equality (<span class="math inline">d(x_1, x_3) \le d(x_1, x_2) + d(x_2, x_3)</span>). Some examples of distance metrics are the <span class="math inline">L_2</span> norm (Euclidean distance, square root of sum of squares), <span class="math inline">L_1</span> norm (Manhattan distance, sum of absolute values), and <span class="math inline">L_\infty</span> norm (Chebyshev distance, the max value). In general, the <span class="math inline">L_p</span> norm of a vector <span class="math inline">\vec x</span> is <span class="math inline">(\sum_i \abs{x_i}^p)^{\frac 1 n}</span>.</p>
<p>The nearest-neighbor rule doesn't need any training, but in general takes <span class="math inline">O(nd)</span> space to store the entire training set (<span class="math inline">n</span> is number of points, <span class="math inline">d</span> is number of dimensions).</p>
<p>If we implement nearest-neighbor naively, it takes <span class="math inline">O(nd)</span> time to get a prediction. We can do better by constructing a Voronoi diagram - a diagram where the space is partitioned into <span class="math inline">n</span> areas, each of those <span class="math inline">n</span> areas contains exactly one point, and the point is always the nearest neighbor within its associated area. In 2D, this takes <span class="math inline">O(n \log n)</span> time and <span class="math inline">O(n)</span> space. In general, this takes <span class="math inline">n^{O(d)}</span> space and <span class="math inline">O(d \log n)</span> query time, which is still rather bad. In the real world, we often use faster, approximate nearest-neighbor algorithms that take advantage of hashing and similar techniques.</p>
<p>Nearest-neighbor is affected by normalization if it distorts the distance metric, such as if we scale one dimension but not another. Usually, we normalize every feature vector by subtracting the mean of all the feature vectors and then dividing by the standard deviation of all the feature vectors.</p>
<p>Nearest-neighbor depends heavily on the distance metric. There's actually a way to determine a good metric to use based on the training set. by first using the the Mahalanobias distance <span class="math inline">d_M(x_1, x_2) = \sqrt{(\vec x_1 - \vec x_2) \cdot M (\vec x_1 - \vec x_2)}</span> where <span class="math inline">M \in \mb{S}_+</span>. This is equivalent to <span class="math inline">M = L L^T</span> for some <span class="math inline">d</span> by <span class="math inline">h</span> matrix <span class="math inline">L</span>. In other words, we're projecting <span class="math inline">\vec x_1 - \vec x_2</span> into a smaller-dimensional space <span class="math inline">h</span> with <span class="math inline">\vec x = L^T(\vec x_1 - \vec x_2)</span>, then taking the Euclidean norm of that point within that space ;wip</p>
<p>The <span class="math inline">k</span>-nearest-neighbor is the neighbor rule, but instead of just finding the nearest neighbor, we find the <span class="math inline">k</span> nearest neighbors in the training set, and then take their results and combine them, usually either taking the mode (majority vote) or mean of all those <span class="math inline">y</span> values (usually <span class="math inline">k</span> is odd, so for boolean <span class="math inline">y</span> there is never a tie).</p>
<p>The <span class="math inline">k</span> in <span class="math inline">k</span>-nearest-neighbors sort of acts like a regularization parameter - the higher <span class="math inline">k</span> is, the less complicated the resulting model is (the decision boundaries in the voronoi diagram are smoother). Intuitively, the higher <span class="math inline">k</span> is, the more points we have to change to get the prediction for a query point to change.</p>
<p>We want to pick a <span class="math inline">k</span> that avoids overfitting, but doesn't make the resulting predictions overly simple. Usually this is done by just trying a bunch of <span class="math inline">k</span> values and evaluating using cross-validation.</p>
<p><span class="math inline">k</span>-nearest-neighbors works surprisingly well on problems with a low number of dimensions and a large training set, and is often used in real-world problems. It works less well with higher dimensionality or smaller training sets.</p>
<p>As the size of the training set tends to infinity, <span class="math inline">P(Y_1 \ne Y \mid X) \le 2P^* - \frac{c}{c - 1} (P^*)^2</span> (Cover &amp; Hart, 1967). <span class="math inline">P(Y_1 \ne Y \mid X)</span> is the probability of a misprediction, <span class="math inline">c</span> is the number of categories/classes, and <span class="math inline">P^*</span> is the Bayes error (the theoretical minimum error, for any machine learning model). Essentially, this is saying that 1-nearest-neighbor will have an error within twice the theoretical minimum error - a very useful result for bounding the error!</p>
<h1 id="section-6">28/9/17</h1>
<p>Review of the last class.</p>
<p>The <strong>Bayes error</strong> is defined by <span class="math inline">P^* = \min_{f : \mathcal{X} \to \set{-1, 1}} P(f(X) \ne Y)</span>. Here, <span class="math inline">X</span> is a feature vector, <span class="math inline">f(X)</span> is the classifier's prediction, and <span class="math inline">Y</span> is the true value. Essentially, it's the minimum error rate for any possible classifier <span class="math inline">f(X)</span>, regardless of how that classifier is implemented.</p>
<p>The <strong>Bayes rule</strong> is the best possible classifier, since it achieves an error equivalent to the Bayes error. ;wip: write down the rule</p>
<p>Derivation:</p>
<blockquote>
<p>Clearly, <span class="math inline">P(f(X) \ne Y) = 1 - P(f(X) = Y)</span>. We don't know <span class="math inline">Y</span>, but we do know it's either -1 or 1, so <span class="math inline">P(f(X) \ne Y) = 1 - P(f(X) = 1 \land Y = 1) - P(f(X) = -1 \land Y = -1)</span>.<br />
We now have unknown <span class="math inline">X</span> and unknown <span class="math inline">Y</span>. We're going to fix <span class="math inline">X</span> and look at the probability of <span class="math inline">Y</span>, a technique known as <strong>conditioning</strong>. This works because <span class="math inline">P(f(X) = 1, Y = 1) = \int_{f(x) = 1, y = 1} P(x \land y) \dee x \dee y = \int_{f(x) = 1, y = 1} P(y \mid x) P(x) \dee x \dee y = \int_{f(x) = 1} \left(\int_{y = 1} P(y \mid x) \dee y \right) \dee x = E(1_{f(X) = 1} P(Y = 1 \mid X))</span>. Here, <span class="math inline">1_{f(X) = 1}</span> is an <strong>indicator function</strong> - a function that is 1 if <span class="math inline">f(X) = 1</span>, and 0 otherwise.<br />
So <span class="math inline">P(f(X) \ne Y) = 1 - E[P(f(X) = 1, Y = 1 \mid X)] = 1 - E(1_{f(X) = 1} P(Y = 1 \mid X)) - E(1_{f(X) = -1} P(Y = -1 \mid X))</span> - we conditioned on <span class="math inline">X</span> and took the expectation.<br />
Let <span class="math inline">\eta(X) = P(Y = 1 \mid X)</span>, so <span class="math inline">1 - \eta(X) = P(Y = -1 \mid X)</span>. Clearly, <span class="math inline">1_{f(X) = -1} = 1 - 1_{f(X) = -1}</span>. ;wip: copy the rest from the slides So <span class="math inline">P(f(X) \ne Y) = E(\eta(X) + 1_{f(X) = 1}(1 - 2 \eta(X)))</span>.</p>
</blockquote>
<p>Note that <span class="math inline">\eta(X)</span> is already defined, but we can choose our own <span class="math inline">1_{f(X) = 1}</span>. Since we want to minimize <span class="math inline">P(f(X) \ne Y)</span>, we want <span class="math inline">1_{f(X) = 1}</span> to be 1 exactly when <span class="math inline">1 - 2 \eta(X) \le 0</span>. This tells us that the optimal classifier <span class="math inline">f^*</span> is <span class="math inline">f^* = \begin{cases} 1 &amp;\text{if } \eta(X) \ge \frac 1 2 \\ -1 &amp;\text{otherwise} \end{cases}</span>.</p>
<p>Since we won't have <span class="math inline">\eta(X) = P(Y = 1 \mid X)</span> in most real-world problems, it's generally not possible to implement this in practice.</p>
<p>For more than two classes, we can do something similar to get <span class="math inline">f^*(X) = \argmax_{1 \le m \le c} P(Y = m \mid X)</span>, and the new Bayes rule would be <span class="math inline">P^* = E(1 - \max_{1 \le m \le c} P(Y = m \mid X))</span>. This is the best we can possibly do assuming the data is sampled independently and identically from <span class="math inline">\tup{X, Y}</span>.</p>
<p>Note that in the worst case, <span class="math inline">P^* = \frac{c - 1}{c}</span>. So the more classes we have, the larger the maximum possible Bayes error is.</p>
<p>So looking back at the formula we saw last class, as the training set size goes to infinity for 1-nearest-neighbor, <span class="math inline">P(Y_1 \ne Y \mid X) \le 2P^* - \frac{c}{c - 1} (P^*)^2</span>. So in the worst case, 1-nearest-neighbor will be less than twice as worse at the Bayes error. For this to be roughly true in practice though, <span class="math inline">n</span> must grow exponentially with respect to <span class="math inline">d</span>, which is generally not practical.</p>
<p>The error rate for <span class="math inline">k</span>-nearest-neighbor for some <span class="math inline">k = 2t + 1</span> is <span class="math inline">\frac 1 {2^n} \sum _{0 \le i \le t} {n \choose i}</span> (this simplifies to <span class="math inline">\frac 1 {2^n}</span> for 1-NN). This tells us that a larger <span class="math inline">k</span> is actually going to give us a higher error in the worst case. Consider how this worst case occurs, when all the points are in random classes.</p>
<p>We want to use a smaller <span class="math inline">k</span> when the classes are easier to separate, and a larger <span class="math inline">k</span> when the classes tend to be harder to separate - larger values for &quot;harder&quot; problems, smaller for &quot;easier&quot; problems. We usually choose <span class="math inline">k</span> via cross-validation.</p>
<p>SSBD page 224: For any <span class="math inline">c &gt; 1</span> and any learning algorithm <span class="math inline">L</span>, there exists a distribution <span class="math inline">\set{0, 1}^d \times \set{0, 1}</span> such that the Bayes error is 0 but for sample sizes <span class="math inline">n \le \frac{(c + 1)^d}{2}</span>, the error probability for <span class="math inline">L</span> is greater than <span class="math inline">\frac 1 4</span>. Essentially, there always exists a distribution such that a given learning algorithm will be wrong 25% or more of the time, but an ideal classifier would get it perfectly, with a limited training set size.</p>
<p>This is especially applicable to <span class="math inline">k</span>-NN, and we often try to avoid this by increasing training set size and doing dimensionality reduction (e.g., projecting a higher dimension into a lower dimension while preserving the most important elements of the features).</p>
<p>;wip: locally linear slide?</p>
<p>For regression, we can also use something similar to <span class="math inline">k</span>-nearest-neighbors. Instead of the training set points having discrete classes, they instead have real values. Essentially, given a feature vector <span class="math inline">\vec x</span>, we take the <span class="math inline">k</span>-nearest-neighbors in the training set, and average their <span class="math inline">y</span> values to get the prediction <span class="math inline">y&#39;</span>. We might additionally do things like do a weighted average based on their distance to <span class="math inline">\vec x</span>.</p>
<h2 id="hard-margin-support-vector-machine-svm">Hard-Margin Support Vector Machine (SVM)</h2>
<p>Recall that the naive perceptron algorithm assumes that the data is linearly separable, and requires that to be true to converge. Furthermore, the perceptron will find any separating hyperplane, depending on how we feed in the data, rather than what we might consider the &quot;best&quot; separating hyperplane.</p>
<p>The perceptron is solving the minimization problem &quot;minimize <span class="math inline">0</span> subject to <span class="math inline">y_i(\vec w^T \vec x_i + b) &gt; 0</span> for all <span class="math inline">i</span>&quot; - there's no objective function, so we're just finding a feasible solution. The SVM also finds a feasible solution, but also tries to maximize the <strong>margin</strong> - the minimum distance from the hyperplane to any training set point.</p>
<p>Given a separating hyperplane <span class="math inline">H</span> defined by <span class="math inline">\vec w \cdot \vec x + b = 0</span>, we want to translate and rotate it until the margin is maximized. First we'll normalize the scale: <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = 0</span>. We translate the hyperplane by changing <span class="math inline">b</span>, and rotate it by changing <span class="math inline">\vec w</span>. Suppose we translate it upward by <span class="math inline">s</span> until it hits a point, and downward by <span class="math inline">t</span> until it hits a point.</p>
<p>Now we have the hyperplanes <span class="math inline">H_1</span> defined by <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = t</span> and <span class="math inline">H_{-1}</span> defined by <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = s</span>, both touching points and having an empty space between them.</p>
<p>Clearly, the distance between <span class="math inline">H</span> and <span class="math inline">H_1</span> is . Clearly, we maximize the distance by ;wip: put into formula slide</p>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
